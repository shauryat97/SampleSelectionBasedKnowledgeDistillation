# SampleSelectionBasedKnowledgeDistillation

Knowledge distillation is an effective approach for compressing deep neural networks by distilling the generalization capability (“dark knowledge”) of a complex network(teacher) to a simpler network(student).
The core idea of KD was given by Geoffrey Hinton et al. in 2014.[1].
The dataset used for KD (transfer set) is generally the original training set on which the teacher model is trained in traditional KD approaches. However, this is a limitation nowadays since the original training set is not publicly available due to privacy, storage costs, sensitiveness of data, etc.
Popular pre-trained models are released nowadays without providing access to training data for example Facebook’s Deepface model which is trained on 4million confidential face images.
The existing answers to this limitation is to either compose a synthetic transfer set that represents the original training set using an iterative method [2] or to use GAN-like generative models [3] to compose a transfer set. But the problem with above mentioned  approaches is that they are computationally expensive and involve complex optimization. The transfer set (ex images) composed after using these approaches is observed to be quite dissimilar to the original training set in terms of both visual similarity and similarity in data manifolds, but still they were  able to transfer the knowledge quite well. This motivated Gaurav et al [4] to investigate arbitrary transfer sets that are unrelated to the training set for the task of KD. They also found out that if transfer sets are “target-class balanced” and somehow related to the original training set  leads to increased distillation performance. 
The past research on using arbitrary transfer sets though does not look at sample level , i.e., we can devise a sample selection metric for creating the transfer set. So , as a part of this project I will first try to implement the paper discussed by Gaurav et al (code not publicly available) . I will then modify the algorithm which was used for creating transfer sets by adding a sample selection threshold metric and thus explore the idea of looking at sample level instead of dataset level while creating these arbitrary transfer sets.

