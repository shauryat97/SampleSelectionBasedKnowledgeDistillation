# Sample Selection Based Knowledge Distillation

Knowledge distillation is an effective approach for compressing deep neural networks by distilling the generalization capability (“dark knowledge”) of a complex network (teacher) to a simpler network (student).
The core idea of KD was given by Geoffrey Hinton et al. in 2014.[1].

# Task 
To introduce a metric for `selective sampling` from arbitrary datasets in order to  increase the distillation performance of the teacher model.

# Baseline
My baseline was based on the [paper](https://arxiv.org/abs/2011.09113) and work done by Gaurav et al on Data Free Knowledge Distillation.
